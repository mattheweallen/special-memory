{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Baby's First Language Model\"\n",
    "author: \"Matt Allen\"\n",
    "date: \"2024-05-04\"\n",
    "categories: [ai, language model]\n",
    "image: \"MLP.png\"\n",
    "\n",
    "format:\n",
    "    html:\n",
    "        code-fold: true\n",
    "jupyter: python\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # from making figures\n",
    "%matplotlib inline\n",
    "from fastbook import *\n",
    "\n",
    "generator_seed = 37"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This piece is an introduction to language models by way of the paper [A Neural Probabilistic Language Model](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf). The paper develops a Multilayer Perceptron (MLP) with learned distributed feature vectors for each word. Nowadays the distributed feature vectors are called embeddings. Embeddings are a solution to the curse of dimensionality i.e. the model will be able to group similar concepts in a vector space to generalize better. As the paper states, it fights the curse of dimensionality with its own weapons. The training sentences inform the model about a combinatorial number of other sentences. In the context of this post, the training sentences are baby names from the Social Security Administration.\n",
    "\n",
    "We will start with a bigram model, which is a special case of [n-gram language models](https://en.wikipedia.org/wiki/Word_n-gram_language_model). An n-gram model uses n-1 tokens to predict the next token. It is a Statistical language model that uses counts of the previous character combinations to predict the next token. We will compare this to a simple Neural Network with a single linear layer and then go onto develop an MLP with embeddings. We will be able to use these models as Generative AI to create new name like words.\n",
    "\n",
    "The MLP architecture was replaced by Recurrent Neural Networks which were replaced by LSTMs which were replaced by Transformers. However, the language modeling framework developed in this paper is still used today. Furthermore, MLP layers are alternated between attention layers in the Transformer architecture of modern LLMs. Also, the fundamentals of tokenization, embeddings, hyperparameters and training loops remain. MLPs are a good place to start in language modeling, because they are easier to understand than transformers and are still trainable with smaller compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The data are first names registered with the Social Security Administration (SSA) from the year of birth 1880 to 2022. Each row contains a name, gender and number of SSA registrations with that name. Here is an example row:\n",
    "\n",
    "Stephanie,F,22775\n",
    "\n",
    "A zip file was downloaded that contains data across years 1880 to 2022. Each file contains one year. All the  files across those years were read and the name was pulled out of the row and changed to be all lowercase without distinguishing between gender, year or popularity. Here is an example row after data wrangling from the file yob1991.txt to be used in the models:\n",
    "\n",
    "stephanie\n",
    "\n",
    "All the unique names across all the years were combined into a single file called names_1880_To_2022.txt, so that the data wrangling step just needs to be done once and then the data can be read from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this step does the data wrangling.\n",
    "# get the data into a reusable format\n",
    "# use the output file of this step to build examples for the model\n",
    "\n",
    "# set wrangle_data to True if you haven't created names_1880_To_2022.txt yet.\n",
    "# the data was downloaded and unzip from https://www.ssa.gov/OACT/babynames/names.zip\n",
    "# the names folder is at the same level in the file system as this notebook.\n",
    "wrangle_data = False\n",
    "\n",
    "# https://stackoverflow.com/questions/3207219/how-do-i-list-all-files-of-a-directory\n",
    "def get_filepaths(directory):\n",
    "    \"\"\"\n",
    "    This function will generate the file names in a directory \n",
    "    tree by walking the tree either top-down or bottom-up. For each \n",
    "    directory in the tree rooted at directory top (including top itself), \n",
    "    it yields a 3-tuple (dirpath, dirnames, filenames).\n",
    "    \"\"\"\n",
    "    file_paths = []  # List which will store all of the full filepaths.\n",
    "\n",
    "    # Walk the tree.\n",
    "    for root, directories, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            # Join the two strings in order to form the full filepath.\n",
    "            filepath = os.path.join(root, filename)\n",
    "            file_paths.append(filepath)  # Add it to the list.\n",
    "\n",
    "    return file_paths  # Self-explanatory.\n",
    "\n",
    "\n",
    "if(wrangle_data):\n",
    "    # Run the above function and store its results in a variable. \n",
    "    # Get all the files paths in the names folder.  \n",
    "    full_file_paths = get_filepaths(\"names\")\n",
    "    # number of files\n",
    "    number_of_files = len(full_file_paths)\n",
    "\n",
    "    # put all the names into an array. make them all lowercase\n",
    "    all_names = []\n",
    "\n",
    "    for f in full_file_paths:\n",
    "        if f.endswith(\".txt\"):\n",
    "            names_split = open(f).read().splitlines()\n",
    "            all_names.extend([line.split(',')[0].lower() for line in names_split])\n",
    "\n",
    "    # collect some stats on the data\n",
    "    number_of_names = len(all_names)\n",
    "    unique_names = list(set(all_names))\n",
    "    number_of_unique_names = len(unique_names)\n",
    "\n",
    "    # save the unique names to a file\n",
    "    with open('names_1880_To_2022.txt', 'w') as f:\n",
    "        f.write('\\n'.join(unique_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read in and look at the data. Below you can see that the shortest names are two letters and the longest are 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of names: 102449\n",
      "min length word: 2 max length word: 15\n",
      "shortest_names: ['ii', 'ja', 'vi', 'od', 'kd', 'ax', 'jd', 'jp', 'st', 'sa', 'rc', 'jt', 'xi', 'ju', 'zy', 'mi', 'kj', 'cj', 'ho', 'se', 'io', 'ge', 'eh', 'jw', 'un', 'kc', 'no', 'an', 'mr', 'va', 'oz', 'du', 'ji', 'ah', 'tr', 'mc', 'si', 'zi', 'ld', 'go', 'pj', 'la', 'qi', 'jm', 'or', 'bj', 'sy', 'lu', 'ao', 'zo', 'su', 'ed', 'xu', 'za', 'ra', 'bb', 'na', 'ry', 'ki', 'pa', 'gy', 'md', 'vu', 'fu', 'ti', 'lj', 'jo', 'ad', 'ej', 'di', 'jl', 'my', 'ku', 'mu', 'lc', 'vy', 'te', 'ar', 'aj', 'ze', 'rb', 'ly', 'jc', 'el', 'so', 'ya', 'ma', 'gi', 'ia', 'yu', 'po', 'li', 'ac', 'lb', 'sj', 'tu', 'ke', 'fe', 'ro', 'kt', 'dj', 'al', 'eb', 'wa', 'mj', 'ab', 'oh', 'rj', 'tc', 'je', 'hy', 'lg', 'yi', 'om', 'yy', 'oc', 'ty', 'me', 'ko', 'av', 'ny', 'ng', 'yo', 'ai', 'jb', 'ka', 'jj', 'ru', 'ea', 'ni', 'ky', 'da', 'rd', 'de', 'le', 'bo', 'do', 'ta', 'rl', 'jr', 'ye', 'in', 'mo', 'ok', 'wc', 'hu', 'wm', 'ha', 'bg', 'ba', 'be', 'lo', 'cy', 'tj', 'en']\n",
      "longest_names: ['laurenelizabeth', 'ryanchristopher', 'christianjoseph', 'sophiaelizabeth', 'mariadelosangel', 'michaelchristop', 'ashleyelizabeth', 'johnchristopher', 'muhammadibrahim', 'jordanalexander', 'joshuaalexander', 'christophermich', 'christopherpaul', 'christianmichae', 'christianalexan', 'jonathanmichael', 'christiandaniel', 'davidchristophe', 'gabrielalexande', 'christopherdavi', 'mariadelrosario', 'christopherjose', 'christopherjohn', 'jordanchristoph', 'markchristopher', 'seanchristopher', 'christopheranth', 'kevinchristophe', 'christopherjame', 'jaydenalexander', 'christiananthon', 'christopherryan', 'muhammadmustafa', 'franciscojavier', 'hannahelizabeth', 'christianjoshua', 'matthewalexande']\n",
      "random sample of 5 names:  ['amirea', 'joslen', 'yazira', 'alaysa', 'aya']\n"
     ]
    }
   ],
   "source": [
    "ssa_names = open('names_1880_To_2022.txt').read().splitlines()\n",
    "\n",
    "total_number_names = len(ssa_names)\n",
    "print(\"total number of names:\", total_number_names)\n",
    "\n",
    "min_len_word = min(len(w) for w in ssa_names)\n",
    "max_len_word = max(len(w) for w in ssa_names)\n",
    "print(\"min length word:\", min_len_word, \"max length word:\", max_len_word)\n",
    "\n",
    "shortest_names = [w for w in ssa_names if len(w) <= min_len_word]\n",
    "longest_names = [w for w in ssa_names if len(w) >= max_len_word]\n",
    "print(\"shortest_names:\", shortest_names)\n",
    "print(\"longest_names:\", longest_names)\n",
    "\n",
    "#random.seed(generator_seed) # uncomment if you want the names to always be the same.\n",
    "print(\"random sample of 5 names: \", random.choices(ssa_names, k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reading the data from the file, we tokenize the data. Tokenization is a subject in itself. We will create a very simple tokenizer. The tokenizer creates a vocabulary of 26 lower case letters of the alphabet plus a '.'. The '.' is used as a special character used to mark the beginning and end of names.\n",
    "\n",
    "We build the tokenizer vocabulary by concatenating all the names together with no spaces and then create an ordered list of unique characters. This ends up covering all 26 lowercase letters in the English alphabet. We add the '.' character to this list.\n",
    "\n",
    "We create two mappings. One that encodes the characters to numbers and one that decodes numbers to characters. We need to convert the words to numbers, because that is the language of computers. We convert numbers back to characters, so that English speaking humans can understand it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: ['.', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "stoi: {'.': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n",
      "itos: {0: '.', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n"
     ]
    }
   ],
   "source": [
    "# tokenizer: the tokens are '.' + lowercase alphabet\n",
    "tokens = ['.'] + sorted(list(set((''.join(ssa_names)))))\n",
    "print(\"tokens:\", tokens)\n",
    "\n",
    "# token to int converter\n",
    "stoi = {s:i for i,s in enumerate(tokens)} # string to int\n",
    "print(\"stoi:\", stoi)\n",
    "# int to token converter\n",
    "itos = {i:s for s,i in stoi.items()} # int to string\n",
    "print(\"itos:\", itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create a function that will build the inputs and outputs that we will feed to the models. The inputs and outputs are created with names that have been shuffled and split into Training, Development and Test sets. The training set is used to train our model by updating its weights. We use the development/validation set to tune hyperparameters. Finally, we use the Test set sparingly. Ideally it is used only once. The performance of the model is based on the Test set. That is the number we would report in our paper and show off to our friends and family.\n",
    "\n",
    "The concept is that we want the model to perform well on unseen data. If we were to use Training set metrics to report the performance of our model, our model could simply memorize the training data to get the possible performance, but it would not do well on new data. Everytime we use a set of data to calculate model performance, the model is learning something from it and it starts to fit to it, but again we want to evaluate our models on data that it has not seen.\n",
    "\n",
    "We could wind up in the scenario that the Test Set performance is not good. Unfortunately, that means we may have to start over. It is not a good situation, but it is better to know.\n",
    "\n",
    "The function also takes a block_size value. This parameter determines the size of the context. That is the number of characters of input we use to predict the next character. In a Bigram model for example, the block_size is 1.\n",
    "\n",
    "Each name training data contains several examples. For example with a block_size of 1, the name matt contains five examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block size is the context length\n",
    "def build_dataset(words, block_size = 1, verbose = False): \n",
    "    X, Y = [], []\n",
    "    \n",
    "    ndex = 0\n",
    "    for w in words:\n",
    "        if verbose and ndex < 3:\n",
    "            print(w)\n",
    "            print(\"input ---> output\")\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            if verbose and ndex < 3:\n",
    "                # pretty print first three names\n",
    "                print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "                \n",
    "            context = context[1:] + [ix] # crop and append\n",
    "\n",
    "        ndex = ndex + 1\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matt\n",
      "input ---> output\n",
      ". ---> m\n",
      "m ---> a\n",
      "a ---> t\n",
      "t ---> t\n",
      "t ---> .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0],\n",
       "         [13],\n",
       "         [ 1],\n",
       "         [20],\n",
       "         [20]]),\n",
       " tensor([13,  1, 20, 20,  0]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_dataset(['matt'], block_size = 1, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
