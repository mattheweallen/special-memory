{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Baby's First Language Model\"\n",
    "author: \"Matt Allen\"\n",
    "date: \"2024-05-04\"\n",
    "categories: [ai, language model]\n",
    "image: \"MLP.png\"\n",
    "\n",
    "format:\n",
    "    html:\n",
    "        code-fold: true\n",
    "jupyter: python\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # from making figures\n",
    "%matplotlib inline\n",
    "from fastbook import *\n",
    "\n",
    "generator_seed = 37"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This piece is an introduction to language models by way of the paper [A Neural Probabilistic Language Model](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf). The paper develops a Multilayer Perceptron (MLP) with learned distributed feature vectors for each word. Nowadays the distributed feature vectors are called embeddings. Embeddings are a solution to the curse of dimensionality i.e. the model will be able to group similar concepts in a vector space to generalize better. As the paper states, it fights the curse of dimensionality with its own weapons. The training sentences inform the model about a combinatorial number of other sentences. In the context of this post, the training sentences are baby names from the Social Security Administration.\n",
    "\n",
    "We will start with a bigram model, which is a special case of [n-gram language models](https://en.wikipedia.org/wiki/Word_n-gram_language_model). An n-gram model uses n-1 tokens to predict the next token. It is a Statistical language model that uses counts of the previous character combinations to predict the next token. We will compare this to a simple Neural Network with a single linear layer and then go onto develop an MLP with embeddings. We will be able to use these models as Generative AI to create new name like words.\n",
    "\n",
    "The MLP architecture was replaced by Recurrent Neural Networks which were replaced by LSTMs which were replaced by Transformers. However, the language modeling framework developed in this paper is still used today. Furthermore, MLP layers are alternated between attention layers in the Transformer architecture of modern LLMs. Also, the fundamentals of tokenization, embeddings, hyperparameters and training loops remain. MLPs are a good place to start in language modeling, because they are easier to understand than transformers and are still trainable with smaller compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The data are first names registered with the Social Security Administration (SSA) from the year of birth 1880 to 2022. Each row contains a name, gender and number of SSA registrations with that name. Here is an example row:\n",
    "\n",
    "Stephanie,F,22775\n",
    "\n",
    "A zip file was downloaded that contains data across years 1880 to 2022. Each file contains one year. All the  files across those years were read and the name was pulled out of the row and changed to be all lowercase without distinguishing between gender, year or popularity. Here is an example row after data wrangling from the file yob1991.txt to be used in the models:\n",
    "\n",
    "stephanie\n",
    "\n",
    "All the unique names across all the years were combined into a single file called names_1880_To_2022.txt, so that the data wrangling step just needs to be done once and then the data can be read from the file.\n",
    "\n",
    "After reading the data from the file, we tokenize the data. Tokenization is a subject in itself. We will create a very simple tokenizer. The tokenizer creates a vocabulary of 26 lower case letters of the alphabet plus a '.'. The '.' is used as a special character used to mark the beginning and end of names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this step does the data wrangling.\n",
    "# get the data into a reusable format\n",
    "# use the output file of this step to build examples for the model\n",
    "\n",
    "# set wrangle_data to True if you haven't created names_1880_To_2022.txt yet.\n",
    "# the data was downloaded and unzip from https://www.ssa.gov/OACT/babynames/names.zip\n",
    "# the names folder is at the same level in the file system as this notebook.\n",
    "wrangle_data = False\n",
    "\n",
    "# https://stackoverflow.com/questions/3207219/how-do-i-list-all-files-of-a-directory\n",
    "def get_filepaths(directory):\n",
    "    \"\"\"\n",
    "    This function will generate the file names in a directory \n",
    "    tree by walking the tree either top-down or bottom-up. For each \n",
    "    directory in the tree rooted at directory top (including top itself), \n",
    "    it yields a 3-tuple (dirpath, dirnames, filenames).\n",
    "    \"\"\"\n",
    "    file_paths = []  # List which will store all of the full filepaths.\n",
    "\n",
    "    # Walk the tree.\n",
    "    for root, directories, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            # Join the two strings in order to form the full filepath.\n",
    "            filepath = os.path.join(root, filename)\n",
    "            file_paths.append(filepath)  # Add it to the list.\n",
    "\n",
    "    return file_paths  # Self-explanatory.\n",
    "\n",
    "\n",
    "if(wrangle_data):\n",
    "    # Run the above function and store its results in a variable. \n",
    "    # Get all the files paths in the names folder.  \n",
    "    full_file_paths = get_filepaths(\"names\")\n",
    "    # number of files\n",
    "    number_of_files = len(full_file_paths)\n",
    "\n",
    "    # put all the names into an array. make them all lowercase\n",
    "    all_names = []\n",
    "\n",
    "    for f in full_file_paths:\n",
    "        if f.endswith(\".txt\"):\n",
    "            names_split = open(f).read().splitlines()\n",
    "            all_names.extend([line.split(',')[0].lower() for line in names_split])\n",
    "\n",
    "    # collect some stats on the data\n",
    "    number_of_names = len(all_names)\n",
    "    unique_names = list(set(all_names))\n",
    "    number_of_unique_names = len(unique_names)\n",
    "\n",
    "    # save the unique names to a file\n",
    "    with open('names_1880_To_2022.txt', 'w') as f:\n",
    "        f.write('\\n'.join(unique_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
